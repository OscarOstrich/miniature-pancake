# -*- coding: utf-8 -*-
"""XGboostCDIstatus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eRvj-eJsx_DmcCSjykE7ppmtUG6VZB8H

# Base Idea:

With MG CDI+ and CDI- normalized filtered taxa data table we are using XGboost from Scikit-Learn to predict CDI status of samples. The goal is to reliably predict the status with our taxa counts .

# MG Filtered Normalized Combined

The first model is XGBoost for predicting Status using both CDI+ and CDI-. I transposed the data table and identified using HC and KR the status of diseased or not, then I added this column to the end of the data so that I could predict that as a variable. I also removed the rows corresponding with 'unknown'.
"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from pandas import read_csv
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.inspection import permutation_importance

url = 'https://raw.githubusercontent.com/OscarOstrich/miniature-pancake/refs/heads/main/norm_filtered_species_table_MT.csv'

df = pd.read_csv(url)

df.head()

#Transposed the table, now create a final column of CDI+ versus CDI-
def assign_status(sample_id):
    if 'HC' in sample_id:
        return 'CDI+'
    elif 'KR' in sample_id:
        return 'CDI-'
    else:
        return 'Unknown'

status_list = [assign_status(col) for col in df.columns]

# Transpose the dataframe so samples are rows, taxa are columns
csv_T = df.transpose()

csv_T['Status'] = status_list
csv_T.head()

x = csv_T.drop('Status', axis=1)
y = csv_T['Status']

y.replace({'CDI+': 1, 'CDI-': 0}, inplace=True)

# Remove rows where Status is 'Unknown'
valid_idx = y[y != 'Unknown'].index

x2 = x.loc[valid_idx]
y2 = y.loc[valid_idx].astype(int)  # Make sure it's integer for classifiers

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)

fold_data = []
for train_idx, test_idx in skf.split(x2, y2):
    x_train_fold, x_test_fold = x2.iloc[train_idx], x2.iloc[test_idx]
    y_train_fold, y_test_fold = y2.iloc[train_idx], y2.iloc[test_idx]

    # Convert columns to numeric, coercing errors, and fill NaN
    x_train_fold = x_train_fold.apply(pd.to_numeric, errors='coerce').fillna(0)
    x_test_fold = x_test_fold.apply(pd.to_numeric, errors='coerce').fillna(0)

    fold_data.append((x_train_fold, x_test_fold, y_train_fold, y_test_fold))

param_grid = {
    'n_estimators': [100,200,300,400,500,600,700,800,900],
    'learning_rate': [0.001, 0.02, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01],
    'max_depth': [1, 2, 3, 4,]
}

xgb_model = XGBClassifier(reg_alpha=0, subsample=0.5, verbosity=0)


grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(x_train_fold, y_train_fold)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Best AUC Score: {best_score}")
